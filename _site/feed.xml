<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-04-15T19:07:12+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Abonia Sojasingarayar</title><subtitle>Holds a Masters degree in Artificial Intelligence and boasts over 7 years of experience. With a robust background in crafting and implementing ML systems across diverse domains, including cybersecurity, banking, transit,cosmetics &amp; hygiene research, and e-commerce.
</subtitle><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><entry><title type="html">How I Passed the GCP Professional Data Engineer Exam</title><link href="http://localhost:4000/blogs/2024-04-09-How-I-Passed-the-GCP-Professional/" rel="alternate" type="text/html" title="How I Passed the GCP Professional Data Engineer Exam" /><published>2024-04-09T00:00:00+02:00</published><updated>2024-04-12T10:28:22+02:00</updated><id>http://localhost:4000/blogs/How-I-Passed-the-GCP-Professional</id><content type="html" xml:base="http://localhost:4000/blogs/2024-04-09-How-I-Passed-the-GCP-Professional/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [GCP, Data Engineering, Cloud, Certification]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#1-basic-concept-from-udemy-courses" id="markdown-toc-1-basic-concept-from-udemy-courses">1. Basic concept from udemy courses</a></li>
  <li><a href="#2google-official-video-coursestheory--practical" id="markdown-toc-2google-official-video-coursestheory--practical">2.Google official video courses(Theory + Practical)</a></li>
  <li><a href="#3-books-and-resource" id="markdown-toc-3-books-and-resource">3. Books and resource</a></li>
  <li><a href="#4-official-documentation" id="markdown-toc-4-official-documentation">4. Official documentation</a></li>
  <li><a href="#5-quizzesexamtopicsmock-test" id="markdown-toc-5-quizzesexamtopicsmock-test">5. Quizzes/ExamTopics/Mock test</a></li>
  <li><a href="#6-cheatsheet" id="markdown-toc-6-cheatsheet">6. <strong><em>Cheatsheet</em></strong></a></li>
  <li><a href="#7-notes" id="markdown-toc-7-notes">7. Notes</a></li>
  <li><a href="#8-tips" id="markdown-toc-8-tips">8. Tips</a></li>
  <li><a href="#9-assessment" id="markdown-toc-9-assessment">9. Assessment</a></li>
  <li><a href="#colclusion" id="markdown-toc-colclusion">Colclusion:</a></li>
  <li><a href="#success-is-easy-to-achieve-once-you-set-your-mind-on-a-specific-goal" id="markdown-toc-success-is-easy-to-achieve-once-you-set-your-mind-on-a-specific-goal">Success is easy to achieve once you set your mind on a specific goal.</a></li>
  <li><a href="#--atticus-aristotle" id="markdown-toc---atticus-aristotle">- Atticus Aristotle</a></li>
</ul>
<p><strong>Table of Contents:</strong></p>
<ul>
  <li><a href="#1-basic-concept-from-udemy-courses">Basic concept from udemy courses</a></li>
  <li><a href="#2google-official-video-coursestheory--practical">Google official video courses(Theory + Practical)</a></li>
</ul>

<p><strong>In this article , I would like to share my experiences for preparing and taking the exam. I hope they’ll help you in one way or another.</strong></p>

<p>Google Cloud Certified Professional Data Engineer exam tests your ability to design, deploy, monitor, and adapt services and infrastructure for data-driven decision-making.</p>

<p>The whole preparation took me nearly 2 months , but with some more focus time I could’ve done it in 1 month. Following steps that I have taken to pass the exam :</p>

<ul>
  <li>
    <p>Basic concept from udemy course</p>
  </li>
  <li>
    <p>Google official video courses(Theory + Practical)</p>
  </li>
  <li>
    <p>Books and resources</p>
  </li>
  <li>
    <p>Official documentation</p>
  </li>
  <li>
    <p>Quizzes/ExamTopics/Mock test</p>
  </li>
  <li>
    <p>Cheatsheet</p>
  </li>
  <li>
    <p>Notes</p>
  </li>
  <li>
    <p>Assessment</p>
  </li>
  <li>
    <p>Tips</p>
  </li>
</ul>

<h3 id="1-basic-concept-from-udemy-courses">1. Basic concept from udemy courses</h3>

<p>I have followed the course <strong><em><a href="https://www.udemy.com/course/google-cloud-professional-data-engineer-get-certified/">Google Cloud Professional Data Engineer: Get Certified 2022</a></em></strong> by Dan Sullivan. I have followed all the classes in this course.As for public it costs 79,99€ and it covers following topics:</p>

<ul>
  <li>
    <p>Build scalable, reliable data pipelines</p>
  </li>
  <li>
    <p>Choose appropriate storage systems, including relational, NoSQL and analytical databases</p>
  </li>
  <li>
    <p>Apply multiple types of machine learning techniques to different use cases</p>
  </li>
  <li>
    <p>Deploy machine learning models in production</p>
  </li>
  <li>
    <p>Monitor data pipelines and machine learning models</p>
  </li>
  <li>
    <p>Design scalable, resilient distributed data intensive applications</p>
  </li>
  <li>
    <p>Migrate data warehouse from on-premises to Google Cloud</p>
  </li>
  <li>
    <p>Evaluate and improve the quality of machine learning models</p>
  </li>
  <li>
    <p>Grasp fundamental concepts in machine learning, such as backpropagation, feature engineering, overfitting and underfitting.</p>
  </li>
</ul>

<p>At the end of the course there will be 50 sample questions to solve which gives you 2 hours as like you are in exam.</p>

<h3 id="2google-official-video-coursestheory--practical">2.Google official video courses(Theory + Practical)</h3>

<p>I recommend to go through all the <strong><em><a href="http://Professional Data Engineer">Professional Data Engineer Google course</a></em></strong> videos and practicals from the official site.</p>

<p>Official guide for the data engineer exam is <a href="https://cloud.google.com/certification/guides/data-engineer">here</a>.</p>

<p>This course covers following topics:</p>

<ul>
  <li>
    <p>Design data processing systems</p>
  </li>
  <li>
    <p>Ensure solution quality</p>
  </li>
  <li>
    <p>Operationalize machine learning models</p>
  </li>
  <li>
    <p>Build and operationalize data processing systems</p>
  </li>
</ul>

<p>It has following 9 major classes with which you can attain a badge from google :</p>
<blockquote>
  <ol>
    <li>Google Cloud Big Data and Machine Learning Fundamentals</li>
    <li>Data Engineering on Google Cloud</li>
    <li>Serverless Data Processing with Dataflow Specialization</li>
    <li>Create and Manage Cloud Resources</li>
    <li>Perform Foundational Data, ML, and AI Tasks in Google Cloud</li>
    <li>Engineer Data in Google Cloud</li>
    <li>Preparing for the Google Cloud Professional Data Engineer Exam</li>
    <li>Professional Data Engineer</li>
    <li>Register for your certification exam</li>
  </ol>
</blockquote>

<p>I have followed the above courses using Partner account so it was completely free and there were free credits available to solve the lab.</p>

<ul>
  <li>
    <p><strong><em><a href="https://linuxacademy.com/course/google-cloud-data-engineer/">Linux Academy course</a></em></strong> (Took 4 weeks + 2 weeks (revision)) — 75% success rate</p>
  </li>
  <li>
    <p><strong><em><a href="https://www.coursera.org/professional-certificates/gcp-data-engineering">Coursera GCP Data Engineering Specialization</a></em></strong> (6 courses)(Took ~4 Weeks) — 20%</p>
  </li>
</ul>

<h3 id="3-books-and-resource">3. Books and resource</h3>

<p>I strongly recommend you to going through <strong><em><a href="https://www.oreilly.com/library/view/official-google-cloud/9781119618430/f07.xhtml#usec0005">Official Google Cloud Certified Professional Data Engineer Study Guide</a></em></strong> [Book] by Dan Sullivan. This study guide offers 100% coverage of every objective for the Google Cloud Certified Professional Data Engineer exam.</p>

<p>This books covers most of the exam topics and there are around 20 quizzes for each chapter.</p>

<h3 id="4-official-documentation">4. Official documentation</h3>

<p>It is also very important to going through the <strong><em><a href="https://cloud.google.com/">official documentation</a></em></strong> to get updated information about the google products and services.</p>

<p><em>For example,sometimes an old mock question’s answers will guide you in a wrong way. Access control on table level is now possible in BigQuery but it wasn’t before.</em> So please dont follow the quiz answers that were provided by default but also I recommend you to do your proper search in official docs , if you have doubt in it.I have used this scenerio as an example and there are other similar cases.</p>

<h3 id="5-quizzesexamtopicsmock-test">5. Quizzes/ExamTopics/Mock test</h3>

<p>Most important part of the preparation is going through as many quizzes as possible and verifing with reasonable answer. To pass the actual exam, you have to spend more time on learning &amp; re-learning through multiple practice tests.</p>

<p>I recommend the following sites for preparing exam:</p>

<ol>
  <li>
    <p><strong><em><a href="https://www.examtopics.com/exams/google/professional-data-engineer/">Examtopics</a></em></strong> : There will be robotic check once after 10 questions and we should pay to get rid of those check and which allow you to focus well in the preparation.Please be aware of wrong answers in Examtopics try to do a search by yourself to assure that its the correct one. Cost : <strong>FREE</strong></p>
  </li>
  <li>
    <p><strong><em><a href="https://www.passnexam.com/google/google-data-engineer/01">PassExam</a></em></strong>: I have also prepared with PassExam which provides right answers for most of the questions with a reason in description and related offical links. Cost : <strong>FREE</strong></p>
  </li>
  <li>
    <p><strong><em><a href="https://www.whizlabs.com/blog/google-cloud-professional-data-engineer-exam-questions/">WhizLabs</a></em></strong> : There are around 25 multiple choice questions with what we can able to get insights on the certification and feel a little more confident. Cost : <strong>FREE</strong></p>
  </li>
  <li>
    <p><strong><em><a href="https://docs.google.com/forms/d/e/1FAIpQLSfkWEzBCP0wQ09ZuFm7G2_4qtkYbfmk_0getojdnPdCYmq37Q/viewform">Sample Questions fom Google</a></em></strong> : There are around 25 sample questions from google at the end of its official course.You can get the answer and explanation once you submit the form. But it is not guaranteed to help you pass the exam.</p>
  </li>
</ol>

<h3 id="6-cheatsheet">6. <strong><em>Cheatsheet</em></strong></h3>

<p>I recommend you to go through the <strong><em><a href="https://github.com/ml874/Data-Engineering-on-GCP-Cheatsheet/blob/master/data_engineering_on_GCP.pdf">Cheatsheet</a></em></strong> once. Cheatsheet is currently a 9-page reference Data Engineering on the Google Cloud Platform. It covers the data engineering lifecycle, machine learning, Google case studies, and GCP’s storage, compute, and big data products.</p>

<p>Going through only this cheatsheet is not guaranteed to help you pass.But it can help you with your revision and refreshing the topics that you have pepared over the period before.</p>

<h3 id="7-notes">7. Notes</h3>

<p>Note taking app : [<strong>Notion](https://www.notion.so)</strong>. The best app for note taking.</p>

<p>Notion has helped us to <strong>revise effectively before exams on all the essential topics.</strong>I recommend to take notes for following topics.</p>

<p>These are the core GCP products that the assessment covers.</p>

<ul>
  <li>
    <p><a href="https://cloud.google.com/ai-platform/docs/technical-overview">AI Platform</a> (+ general AI and ML concepts)</p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/bigquery">BigQuery</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/bigtable">BigTable</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/sql">Cloud SQL</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/dataflow">Dataflow</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/dataproc">Dataproc</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/pubsub">Pub/Sub</a></p>
  </li>
</ul>

<p>But there was some overlap with other Google tools too.</p>

<ul>
  <li>
    <p><a href="https://cloud.google.com/storage">Cloud Storage</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/spanner">Cloud Spanner</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/composer">Composer</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/compute">Compute Engine</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/dlp">Data Loss Prevention</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/dataprep">Dataprep</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/datastore">Datastore</a></p>
  </li>
  <li>
    <p><a href="https://firebase.google.com/docs/firestore">Firestore</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/natural-language">Natural Language AI</a></p>
  </li>
  <li>
    <p><a href="https://www.tensorflow.org/">Tensorflow</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/text-to-speech">Text-to-speech</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/video-intelligence">Video AI</a></p>
  </li>
</ul>

<h3 id="8-tips">8. Tips</h3>

<ul>
  <li>
    <p>Preparation is key to success. so many weekends, many holidays, I spent preparing for this exam.</p>
  </li>
  <li>
    <p>You can eliminate all the answer that recommend <strong>non-GCP solutions</strong>.</p>
  </li>
  <li>
    <p>As with many multiple-choice exams, <strong>eliminate</strong>. If you can’t eliminate to one possible answer, make a guess.</p>
  </li>
</ul>

<h3 id="9-assessment">9. Assessment</h3>

<p>I took the assessment via Kryterion’s <a href="https://www.webassessor.com/googlecloud/">Webassessor</a>. <strong>Exam Duration</strong>: 2 hours, <strong>Registration fee</strong>: $200 (plus tax where applicable) <strong>, Languages</strong>: English, Japanese and <strong>Exam format</strong>: Multiple choice and multiple select taken remotely or in person at a test center.</p>

<p>On exam day, I had to show my surroundings via webcam, turn off my phone, empty my desk, If your name is not visible you should take photo from your mobile a,d then you should show it to the sponser staff at the other end etc. All this took ~ half an hour. I didn’t have to talk to the person behind the camera, but we can chat. The person also monitor you via webcam during the assessment.</p>

<p>I found the questions are bit hard and challenging. In the end, I got a provisional <em>Pass</em>, but it should be confirmed by Google — for whatever reason. That confirmation came 4 –10 days later. So now I am officially Google Certified Professional Data Engineer 🎉 and here is my Official Google Certification!</p>

<p><img src="https://cdn-images-1.medium.com/max/2548/1*73NRxlyIZ8OH_2-CSYK4dQ.png" alt="Image by Author on [Professional Data Engineer Certification](https://www.credential.net/4e184ed8-23eb-4783-810c-a3c3e4e10b7d)" /></p>

<p>Tada 🎉</p>

<p><img src="https://cdn-images-1.medium.com/max/2214/1*-kDC5Id6bejn-wY7aRlHAw.jpeg" alt="Image by Author" /></p>

<p>Got this amazing <strong>Google hoodie</strong> for free from google, after a month from the date of ordering. You can order it via the link that they send you with your official confirmation mail.</p>

<h2 id="colclusion">Colclusion:</h2>

<p>I hope this will help you pass the exam. All in all, you need to have solid Google Cloud knowledge to start with. If you are not familiar with the technology, the advised courses will help you. To get well prepared for the exam, I encourage you to complete the Official Data Engineer course videos and read about the best practices of GCP products, followed by the ML Crash Course provided by Google or Coursera. You should be ready to pass the exam by combining your studies with your knowledge. Good luck!</p>
<blockquote>
  <h1 id="success-is-easy-to-achieve-once-you-set-your-mind-on-a-specific-goal">Success is easy to achieve once you set your mind on a specific goal.</h1>
  <h1 id="--atticus-aristotle">- Atticus Aristotle</h1>
</blockquote>

<p><strong>Wish you the very best with your GCP certifications. You can reach me via:</strong></p>

<blockquote>
  <p><strong>Connect with me on <a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Find me on <a href="https://github.com/Abonia1">Github</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Visit my technical channel on <a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Support: <a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Learn about my journey of preparing for and passing the Google Cloud Certified Professional Data Engineer exam.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/gcp-exam.webp" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/gcp-exam.webp" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deploying a RAG Application in AWS Lambda using Docker and ECR</title><link href="http://localhost:4000/blogs/2024-03-05-Deploying-a-RAG-Application-in-AWS-Lambda-using-Docker-and-ECR/" rel="alternate" type="text/html" title="Deploying a RAG Application in AWS Lambda using Docker and ECR" /><published>2024-03-05T00:00:00+01:00</published><updated>2024-04-12T10:28:22+02:00</updated><id>http://localhost:4000/blogs/Deploying-a-RAG-Application-in-AWS-Lambda-using-Docker-and-ECR</id><content type="html" xml:base="http://localhost:4000/blogs/2024-03-05-Deploying-a-RAG-Application-in-AWS-Lambda-using-Docker-and-ECR/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [AWS, Lambda , ECR ,Docker , LangChain, OpenAI]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#lambda--ecr--docker--langchain--openai" id="markdown-toc-lambda--ecr--docker--langchain--openai">Lambda — ECR — Docker — LangChain — OpenAI</a></li>
  <li><a href="#services-overview" id="markdown-toc-services-overview">Services Overview</a></li>
  <li><a href="#integration-and-deployment-steps" id="markdown-toc-integration-and-deployment-steps">Integration and Deployment Steps</a></li>
  <li><a href="#implementation" id="markdown-toc-implementation">Implementation</a>    <ul>
      <li><a href="#environment-setup" id="markdown-toc-environment-setup">Environment Setup</a></li>
      <li><a href="#data-loading-and-processing" id="markdown-toc-data-loading-and-processing">Data Loading and Processing</a></li>
      <li><a href="#response-generation" id="markdown-toc-response-generation">Response Generation</a></li>
      <li><a href="#aws-lambda-handler" id="markdown-toc-aws-lambda-handler">AWS Lambda Handler</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#thanks-for-reading" id="markdown-toc-thanks-for-reading"><em>Thanks for Reading!</em></a></li>
</ul>

<h3 id="lambda--ecr--docker--langchain--openai">Lambda — ECR — Docker — LangChain — OpenAI</h3>

<p>Deploying a RAG (Retrieval-Augmented Generation) application in AWS Lambda using Docker and Amazon Elastic Container Registry (ECR) with LangChain involves several steps and services. This article will explain each service and how they work together in an integrated way, followed by the steps to deploy it.</p>

<p>Here is the link to the complete tutorial on <strong><a href="https://youtu.be/gicsb9p7uj4?si=9F2l6z1rNpkUOoIR">Deploying RAG in AWS</a></strong>.</p>

<p>Before proceeding further, I would like to kindly suggest that you take some time to read and watch the tutorial titled <strong><a href="https://medium.com/@abonia/build-and-deploy-llm-application-in-aws-cca46c662749">Build and Deploy LLM Application in AWS</a></strong>. This tutorial can provide you with a solid foundation on Lambda LLM application deployment.
<strong><a href="https://medium.com/@abonia/build-and-deploy-llm-application-in-aws-cca46c662749">Build and Deploy LLM Application in AWS</a></strong></p>

<h2 id="services-overview">Services Overview</h2>

<p><img src="https://cdn-images-1.medium.com/max/3840/1*KBp2fuAVMvBfpE10p4nrrQ.png" alt="Architecture Overview — Image by Author" /></p>

<p><a href="https://aws.amazon.com/lambda/getting-started/?gclid=CjwKCAjwzN-vBhAkEiwAYiO7oF7Q4aEK3hKUA0HVPEe3wXKen_tVUDmxun4s5PjasxZ-deFf-j19vxoCovkQAvD_BwE&amp;trk=65546593-a75a-4317-b5dc-80218abfdb10&amp;sc_channel=ps&amp;s_kwcid=AL!4422!3!651542249938!e!!g!!aws%20lambda&amp;ef_id=CjwKCAjwzN-vBhAkEiwAYiO7oF7Q4aEK3hKUA0HVPEe3wXKen_tVUDmxun4s5PjasxZ-deFf-j19vxoCovkQAvD_BwE:G:s&amp;s_kwcid=AL!4422!3!651542249938!e!!g!!aws%20lambda!19835810591!150095231954">AWS Lambda</a>: A serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you. It allows you to run code without provisioning or managing servers.</p>

<ul>
  <li>
    <p><a href="https://aws.amazon.com/ecr/">Amazon ECR</a>: A fully-managed container registry that makes it easy for developers to store, manage, and deploy Docker container images. It’s integrated with Amazon ECS and AWS Fargate, simplifying your development to production workflow.</p>
  </li>
  <li><a href="https://www.docker.com/">Docker</a>: A platform that uses containerization to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package. This ensures that the application runs quickly and reliably on any server.
    <blockquote>
      <p>Dowload Link: <a href="https://www.docker.com/products/docker-desktop/">https://www.docker.com/products/docker-desktop/</a></p>
    </blockquote>
  </li>
  <li><a href="https://www.langchain.com/">LangChain</a>: A framework for building and deploying language models, providing tools for document loading, vector storage, embeddings, and more. It’s used here to facilitate the deployment of the RAG model.</li>
</ul>

<h2 id="integration-and-deployment-steps">Integration and Deployment Steps</h2>

<ol>
  <li><strong>Prepare Your Environment:</strong> Ensure you have the AWS CLI installed and configured, Docker installed, and Python 3.11 or later installed and configured. You’ll also need an active AWS account with the necessary permissions.
    <blockquote>
      <p>Download Link: <a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</a></p>
    </blockquote>
  </li>
  <li>
    <p><strong>Create a Dockerfile:</strong> This file defines the environment in which your Lambda function will run. It starts from a base image provided by AWS (<em><code class="language-plaintext highlighter-rouge">public.ecr.aws/lambda/python:3.12</code></em>) and copies your application code and dependencies into the image. It also installs any necessary Python packages from a <code class="language-plaintext highlighter-rouge">requirements.txt</code> file as below:</p>

    <p>langchain_community
 boto3==1.34.37
 numpy
 langchain
 langchainhub
 langchain-openai
 chromadb
 bs4
 tiktoken
 openai</p>
  </li>
</ol>

<div class="language-docker highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Use the AWS base image for Python 3.12</span>
    FROM public.ecr.aws/lambda/python:3.12
    
    <span class="c"># Install build-essential to get the C++ compiler and other necessary tools</span>
    RUN microdnf update -y &amp;&amp; microdnf install -y gcc-c++ make
    
    <span class="c"># Copy requirements.txt</span>
    COPY requirements.txt ${LAMBDA_TASK_ROOT}
    
    # Install the specified packages
    RUN pip install -r requirements.txt
    
    <span class="c"># Copy function code</span>
    COPY lambda_function.py ${LAMBDA_TASK_ROOT}
    
    # Set the permissions to make the file executable
    RUN chmod +x lambda_function.py
    
    <span class="c"># Set the CMD to your handler</span>
    CMD [ "lambda_function.lambda_handler" ]
</code></pre></div></div>

<ol>
  <li>
    <p><strong>Build and Push Your Docker Image to Amazon ECR:</strong> Use the AWS CLI to create a repository in ECR, build your Docker image, and push it to the repository. This step requires permissions to interact with ECR and S3.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  aws ecr create-repository - repository-name my-rag-lambda
  docker build <span class="nt">-t</span> my-test-lambda <span class="nb">.</span>
  docker tag my-rag-lambda:latest &lt;aws_account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/my-test-lambda:latest
  docker push &lt;aws_account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/my-test-lambda:latest
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Create a Lambda Function</strong>: In the AWS Lambda console, create a new function using the container image option. Specify the URI of the image in ECR as the image source.</p>
  </li>
  <li>
    <p><strong>Configure Your Lambda Function</strong>: Set the necessary environment variables, such as <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> for LangChain, and configure any other settings as needed.</p>
  </li>
  <li>
    <p><strong>Test Your Lambda Function</strong>: Invoke your Lambda function to test it. You can do this from the AWS Lambda console or using the AWS CLI. Ensure that the function is correctly processing inputs and generating outputs as expected.</p>
  </li>
</ol>

<h2 id="implementation">Implementation</h2>

<p>Below code is designed to deploy a Retrieval-Augmented Generation (RAG) model using AWS Lambda, Docker, and Amazon ECR, with LangChain for language model deployment.</p>

<h3 id="environment-setup">Environment Setup</h3>

<p>First, imports necessary libraries and sets up the environment:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import boto3
import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os
# Retrieve the OpenAI API key from environment variables
OPENAI_API_KEY = os.environ['OPENAI_API_KEY']
print(OPENAI_API_KEY)
</code></pre></div></div>

<ul>
  <li>boto3: The AWS SDK for Python, allowing Python developers to write software that makes use of services like Amazon S3, Amazon EC2, and others.</li>
  <li>bs4: Beautiful Soup, a library for pulling data out of HTML and XML files.</li>
  <li>LangChain: A framework for building and deploying language models, providing tools for document loading, vector storage, embeddings, and more.</li>
  <li>Environment Variables: The code retrieves the <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> from the environment variables, which is crucial for accessing OpenAI’s API.</li>
</ul>

<h3 id="data-loading-and-processing">Data Loading and Processing</h3>

<p>The <code class="language-plaintext highlighter-rouge">load_data</code> function is responsible for loading, chunking, and indexing the contents of a web page:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def load_data():
 loader = WebBaseLoader(
 web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
 bs_kwargs=dict(
 parse_only=bs4.SoupStrainer(
 class_=("post-content", "post-title", "post-header")
 )
 ),
 )
 docs = loader.load()
 text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
 splits = text_splitter.split_documents(docs)
 vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
 retriever = vectorstore.as_retriever()
 return retriever
</code></pre></div></div>

<ul>
  <li>WebBaseLoader: Loads documents from web paths, using Beautiful Soup to parse and extract specific elements.</li>
  <li>RecursiveCharacterTextSplitter: Splits documents into chunks based on character count and overlap.</li>
  <li>Chroma: Creates a vector store from the split documents, using OpenAI embeddings for vectorization.</li>
  <li>Retriever: A retriever object that can be used to retrieve relevant documents based on queries.</li>
</ul>

<h3 id="response-generation">Response Generation</h3>

<p>The <code class="language-plaintext highlighter-rouge">get_response</code> function generates a response to a given query using the RAG model:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_response(query):
 prompt = hub.pull("rlm/rag-prompt")
 llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
 retriever = load_data()
 rag_chain = (
 {"context": retriever | format_docs, "question": RunnablePassthrough()}
 | prompt
 | llm
 | StrOutputParser()
 )
 return rag_chain.invoke(query)
</code></pre></div></div>

<ul>
  <li>hub.pull: Retrieves a prompt from <a href="https://smith.langchain.com/hub/rlm/rag-prompt">LangChain’s hub</a>.</li>
  <li>ChatOpenAI: Initializes a chat model with GPT-3.5 Turbo.</li>
  <li>RunnablePassthrough: A component that passes the input directly to the next component in the chain.</li>
  <li>StrOutputParser: Parses the output of the RAG model into a string format.</li>
</ul>

<h3 id="aws-lambda-handler">AWS Lambda Handler</h3>

<p>Finally, the <code class="language-plaintext highlighter-rouge">lambda_handler</code> function is the entry point for AWS Lambda, which receives an event and context, processes the query, and returns a response:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def lambda_handler(event, context):
 query = event.get("question")
 response = get_response(query)
 print("response:", response)
 return {"body": response, "statusCode": 200}
</code></pre></div></div>

<ul>
  <li>
    <p>Event and Context: AWS Lambda passes an event object and a context object to the handler. The event object contains information about the triggering event, and the context object contains information about the runtime environment.</p>
  </li>
  <li>
    <p>Query Processing: The function extracts the query from the event, generates a response using the <code class="language-plaintext highlighter-rouge">get_response</code> function, and prints the response and returns a response object containing the generated response and a status code of 200, indicating success.</p>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Deploying a RAG model in AWS Lambda using Docker and ECR with LangChain involves preparing your environment, creating a Dockerfile, building and pushing your Docker image to ECR, creating a Lambda function, configuring it, and testing it. This process leverages the serverless capabilities of AWS Lambda, the containerization benefits of Docker, and the language model deployment capability provided by LangChain.</p>

<blockquote>
  <p><strong>Connect with me on <a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Find me on <a href="https://github.com/Abonia1">Github</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Visit my technical channel on <a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Support: <a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>
</blockquote>

<blockquote>
  <h1 id="thanks-for-reading"><em>Thanks for Reading!</em></h1>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Deploying a RAG (Retrieval-Augmented Generation) application in AWS Lambda using Docker and Amazon Elastic Container Registry (ECR) with LangChain.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/aws-rag.webp" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/aws-rag.webp" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">EDA — Visualize Embeddings of RAG</title><link href="http://localhost:4000/blogs/2024-01-05-EDA-Visualize-Embeddings-of-RAG/" rel="alternate" type="text/html" title="EDA — Visualize Embeddings of RAG" /><published>2024-01-05T00:00:00+01:00</published><updated>2024-04-12T10:28:22+02:00</updated><id>http://localhost:4000/blogs/EDA%E2%80%94Visualize-Embeddings-of-RAG</id><content type="html" xml:base="http://localhost:4000/blogs/2024-01-05-EDA-Visualize-Embeddings-of-RAG/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [UMAP,RAG,Visualization,Langchain,ChromaHuggingFaceEmbeddings]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#introduction-to-rag" id="markdown-toc-introduction-to-rag">Introduction to RAG</a></li>
  <li><a href="#setting-up-the-environment" id="markdown-toc-setting-up-the-environment">Setting Up the Environment</a></li>
  <li><a href="#loading-the-model-and-vector-store" id="markdown-toc-loading-the-model-and-vector-store">Loading the Model and Vector Store</a></li>
  <li><a href="#fetching-and-preparing-data" id="markdown-toc-fetching-and-preparing-data">Fetching and Preparing Data</a></li>
  <li><a href="#calculating-distances" id="markdown-toc-calculating-distances">Calculating Distances</a></li>
  <li><a href="#visualizing-embeddings" id="markdown-toc-visualizing-embeddings">Visualizing Embeddings</a>    <ul>
      <li><a href="#1-using-spotlight" id="markdown-toc-1-using-spotlight">1. Using Spotlight</a></li>
      <li><a href="#2-using-umap" id="markdown-toc-2-using-umap">2. Using UMAP</a></li>
      <li><a href="#3-using-tensorboard" id="markdown-toc-3-using-tensorboard">3. Using Tensorboard</a></li>
      <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#thanks-for-reading" id="markdown-toc-thanks-for-reading">Thanks for Reading!</a></li>
</ul>

<p>In this article, we delve into the how we can visualize Retrieval Augmented Generation (RAG) data using the langchain framework in conjunction with Hugging Face’s language models and embeddings. We’ll explore how to leverage the <code class="language-plaintext highlighter-rouge">HuggingFaceEmbeddings</code> and <code class="language-plaintext highlighter-rouge">Chroma</code> vector store for efficient embedding and document retrieval, followed by a practical example of visualizing these embeddings to understand their distribution and relevance to a given question.</p>

<p>We’ll explore how to use the Hugging Face Embeddings for embedding text data and store it in chroma and then visualize it using UMAP (Uniform Manifold Approximation and Projection), a dimensionality reduction technique that helps in visualizing high-dimensional data in a more interpretable way.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://cdn-images-1.medium.com/max/2924/1*5nX7pJ5xOoMelSGm67hhHA.gif" alt="Courtesy of Spotlight" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><i>Image Credits - Courtesy of Spotlight</i></td>
    </tr>
  </tbody>
</table>

<h3 id="introduction-to-rag">Introduction to RAG</h3>

<p>RAG is a cutting-edge approach that combines the strengths of pretrained Large Language Models (LLMs) and your own data to generate responses. It retrieves documents, passes them through a sequence-to-sequence model, and then marginalizes to generate outputs. This method is particularly useful for querying specific documents or interacting with your own data in a conversational manner .</p>

<h3 id="setting-up-the-environment">Setting Up the Environment</h3>

<p>To begin, we’ll need to import the necessary libraries and set up our environment. We’ll use <code class="language-plaintext highlighter-rouge">pandas</code> for data manipulation, <code class="language-plaintext highlighter-rouge">langchain.embeddings</code> for handling embeddings, and <code class="language-plaintext highlighter-rouge">langchain.vectorstores</code> for our vector store.
<a href="https://python.langchain.com/docs/get_started/introduction"><strong>Introduction | 🦜️🔗 Langchain</strong>
<em>LangChain is a framework for developing applications powered by language models. It enables applications that:</em>python.langchain.com</a></p>

<p>So do install as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!pip install pandas langchain renumics-spotlight umap-learn

import pandas as pd
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
</code></pre></div></div>

<h3 id="loading-the-model-and-vector-store">Loading the Model and Vector Store</h3>

<p>Next, we’ll specify the model path for our embeddings and create instances of <code class="language-plaintext highlighter-rouge">HuggingFaceEmbeddings</code> and <code class="language-plaintext highlighter-rouge">Chroma</code>. We’ll also configure the model to use the CPU/GPU for computations and ensure embeddings are not normalized for our visualization purposes.</p>

<p>Before dive in we consider, you already have chroma db collection and see how we can visualize it.If no you can create one as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader
from bs4 import BeautifulSoup as Soup
from langchain.vectorstores import Chroma

url = "https://www.freenews.fr/"
loader = RecursiveUrlLoader(
    url=url, max_depth=5, extractor=lambda x: Soup(x, "lxml").text
)
documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=0
        )
texts = text_splitter.split_documents(documents)

modelPath = "sentence-transformers/distiluse-base-multilingual-cased-v1"

model_kwargs = {'device':'cpu'}
#encode_kwargs = {'normalize_embeddings': False}

embeddings = HuggingFaceEmbeddings(model_name=modelPath,model_kwargs=model_kwargs) 

# SAVE
docs_vectorstore = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db_multilingual")
</code></pre></div></div>

<p>Provide the link to your chroma persist_directory below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>modelPath = "sentence-transformers/distiluse-base-multilingual-cased-v1"
model_kwargs = {'device':'cpu'}
encode_kwargs = {'normalize_embeddings': False}
embeddings_model = HuggingFaceEmbeddings(model_name=modelPath, model_kwargs=model_kwargs)
docs_vectorstore = Chroma(persist_directory="./chroma_db_multilingual", embedding_function=embeddings_model)
</code></pre></div></div>

<h3 id="fetching-and-preparing-data">Fetching and Preparing Data</h3>

<p>We’ll retrieve our data from the vector store, including metadata, documents, and embeddings. Then, we’ll create a DataFrame to organize this data and add a column to indicate whether each document contains the answer to our query.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>response = docs_vectorstore.get(include=["metadatas", "documents", "embeddings"])
df = pd.DataFrame({
 "id": response["ids"],
 "source": [metadata.get("source") for metadata in response["metadatas"]],
 "page": [metadata.get("page", -1) for metadata in response["metadatas"]],
 "document": response["documents"],
 "embedding": response["embeddings"],
})
df["contains_answer"] = df["document"].apply(lambda x: "Nœud Répartition Optique)" in x)
</code></pre></div></div>

<h2 id="calculating-distances">Calculating Distances</h2>

<p>To find the closest match to our question, we calculate the Euclidean distance between the question embedding and each document embedding.</p>

<p><img src="https://cdn-images-1.medium.com/max/4408/1*68cE3DuXYssA7tAvtYm7-Q.png" alt="Euclidean Distance" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>question_embedding = embeddings_model.embed_query(question)
df["dist"] = df.apply(
    lambda row: np.linalg.norm(
        np.array(row["embedding"]) - question_embedding
    ),
    axis=1,
)
</code></pre></div></div>

<h2 id="visualizing-embeddings">Visualizing Embeddings</h2>

<h3 id="1-using-spotlight">1. Using Spotlight</h3>

<p>This visualization will help us understand how closely related documents are to our query and identify potential areas of improvement or further investigation.</p>

<p>Finally, we use the spotlight module from renumics to visualize the data. This step is crucial for understanding the distribution of documents in relation to the question and the model’s response.
<strong><a href="https://github.com/Renumics/spotlight">GitHub - Interactively explore unstructured datasets from your dataframe-Interactively explore unstructured datasets from your dataframe</a></strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from renumics import spotlight
spotlight.show(df)
</code></pre></div></div>

<p><img src="https://cdn-images-1.medium.com/v2/resize:fit:6776/1*jOH05LoRaN3TnJwiB9V1Dg.png" alt="UMAP — Visualize the embeddings" /></p>

<p><strong><a href="https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial.ipynb">Notebook-Renumics/rag-demo.Retrieval-Augmented Generation Assistant Demo</a></strong></p>

<h3 id="2-using-umap">2. Using UMAP</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import umap
# Find the  5 closest vectors
closest_vectors_indices = df.nsmallest(5, 'dist')['id'].values

# Prepare the embeddings for UMAP
embeddings = np.array([np.array(x) for x in df["embedding"]])

# Reduce dimensionality with UMAP
reducer = umap.UMAP()
embedding_reduced = reducer.fit_transform(embeddings)

# Plot the reduced embeddings
plt.scatter(embedding_reduced[:,  0], embedding_reduced[:,  1], c='gray', alpha=0.2)

# Highlight the question embedding and the  5 closest vectors
plt.scatter(embedding_reduced[df["id"].isin(closest_vectors_indices),  0], embedding_reduced[df["id"].isin(closest_vectors_indices),  1], c='red', alpha=1)
plt.scatter(embedding_reduced[df["id"] == df[df["dist"] == df["dist"].min()]["id"].values[0],  0], embedding_reduced[df["id"] == df[df["dist"] == df["dist"].min()]["id"].values[0],  1], c='blue', alpha=1, marker='*')

# Add labels and title
plt.title("UMAP Visualization of Text Embeddings with Question Highlighted")
plt.xlabel("UMAP  1")
plt.ylabel("UMAP  2")
plt.show()
</code></pre></div></div>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*5q-fn2cWbzzJP6ielmyPxA.png" alt="UMAP — Visualize Text embedding — Blue point is the Question" /></p>

<h3 id="3-using-tensorboard">3. Using Tensorboard</h3>

<p>Another effective way to visualize embeddings, especially useful for gaining insights into word embeddings and the relationships between them, is by using TensorBoard. TensorBoard is a tool that allows for the visualization of machine learning models and their metrics, including word embeddings, in a user-friendly interface. It can be particularly beneficial when you want to explore the semantic similarities and relationships between words in your embeddings.</p>

<p><strong>Setting Up TensorBoard</strong></p>

<p>First, ensure you have TensorBoard installed. You can install it using pip:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install tensorboard
</code></pre></div></div>

<p><strong>Visualizing Word Embeddings with TensorBoard</strong></p>

<p>To visualize word embeddings using TensorBoard, you’ll typically need to convert your embeddings into a format that TensorBoard can understand. This often involves creating a metadata file that maps words to their embeddings and then using TensorBoard’s <code class="language-plaintext highlighter-rouge">Projector</code> to visualize these embeddings.
<strong><a href="https://www.tensorflow.org/tensorboard/get_started">Get started with TensorBoard | TensorFlow</a></strong></p>

<p>Here’s a simplified step-by-step guide on how to do this:</p>

<ol>
  <li>
    <p>Prepare Your Embeddings: Ensure your embeddings are in a suitable format. For TensorBoard, you might need to convert your embeddings into a <code class="language-plaintext highlighter-rouge">.tsv</code> (Tab-Separated Values) file where each line contains a word and its corresponding embedding vector.</p>
  </li>
  <li>
    <p>Create a Metadata File: Alongside your embeddings file, create a metadata file that maps each word to its index in the embeddings file. This file is also in <code class="language-plaintext highlighter-rouge">.tsv</code> format.</p>
  </li>
  <li>
    <p>Use TensorBoard’s Projector:
 — Start TensorBoard by running <code class="language-plaintext highlighter-rouge">tensorboard — logdir=path/to/your/logs</code> in your terminal.
 — In your web browser, navigate to the TensorBoard interface (usually at <code class="language-plaintext highlighter-rouge">localhost:6006</code>).
 — Go to the <code class="language-plaintext highlighter-rouge">Projector</code> tab.
 — Click on <code class="language-plaintext highlighter-rouge">Load</code> under the <code class="language-plaintext highlighter-rouge">Embeddings</code> section.
 — Upload your embeddings file and metadata file.</p>
  </li>
  <li>
    <p>Explore Your Embeddings:
 — Once your embeddings are loaded, you can explore them in various ways, such as through a 2D or 3D scatter plot, where each point represents a word and its position is determined by its embedding vector.
 — You can also use the <code class="language-plaintext highlighter-rouge">Word</code> search bar to find specific words and see how they are positioned relative to others in the embedding space.</p>
  </li>
</ol>

<p><img src="https://cdn-images-1.medium.com/max/5200/0*Bk4Bf3VD8P2FKl9M" alt="" /></p>

<p>-Semantic Similarity: TensorBoard makes it easier to visually inspect the semantic relationships between words by examining their positions in the embedding space.</p>
<ul>
  <li>Ease of Use: The user-friendly interface of TensorBoard allows for intuitive exploration of embeddings without the need for complex code.</li>
  <li>Insight into Model Performance: Beyond visualizing embeddings, TensorBoard can also be used to track model metrics, network weight distributions, and other performance indicators, providing a comprehensive toolkit for monitoring and understanding your models.</li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>Visualizing RAG data provides valuable insights into the model’s decision-making process, helping us understand how it selects and interprets information from external documents.By visualizing RAG data, we gain valuable insights into the relationships between documents and our queries. This technique not only aids in understanding the performance of our RAG system but also guides us in refining models and datasets for better accuracy and relevance.</p>

<blockquote>
  <h1 id="thanks-for-reading">Thanks for Reading!</h1>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Visualize Retrieval Augmented Generation (RAG) data using the langchain framework in conjunction with Hugging Face’s language models and embeddings and chromaDB.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/EDA-RAG.GIF" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/EDA-RAG.GIF" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Best LLM and LLMOps Resources for 2023</title><link href="http://localhost:4000/blogs/2023-12-05-Best-LLM-and-LLMOps-Resources-for-2023/" rel="alternate" type="text/html" title="Best LLM and LLMOps Resources for 2023" /><published>2023-12-05T00:00:00+01:00</published><updated>2024-04-12T10:28:22+02:00</updated><id>http://localhost:4000/blogs/Best-LLM-and-LLMOps-Resources-for-2023</id><content type="html" xml:base="http://localhost:4000/blogs/2023-12-05-Best-LLM-and-LLMOps-Resources-for-2023/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [Large Language Models, GPT-3, ChatGPT, Books, Courses And Training]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#𝐁𝐞𝐬𝐭-𝐜𝐨𝐮𝐫𝐬𝐞𝐬-𝐛𝐨𝐨𝐤𝐬-𝐚𝐧𝐝-𝐫𝐞𝐬𝐨𝐮𝐫𝐜𝐞𝐬-𝐭𝐨-𝐬𝐭𝐚𝐫𝐭𝐚𝐝𝐯𝐚𝐧𝐜𝐞-𝐲𝐨𝐮𝐫-𝐋𝐋𝐌-𝐚𝐧𝐝-𝐋𝐋𝐌𝐎𝐩𝐬-𝐣𝐨𝐮𝐫𝐧𝐞𝐲" id="markdown-toc-𝐁𝐞𝐬𝐭-𝐜𝐨𝐮𝐫𝐬𝐞𝐬-𝐛𝐨𝐨𝐤𝐬-𝐚𝐧𝐝-𝐫𝐞𝐬𝐨𝐮𝐫𝐜𝐞𝐬-𝐭𝐨-𝐬𝐭𝐚𝐫𝐭𝐚𝐝𝐯𝐚𝐧𝐜𝐞-𝐲𝐨𝐮𝐫-𝐋𝐋𝐌-𝐚𝐧𝐝-𝐋𝐋𝐌𝐎𝐩𝐬-𝐣𝐨𝐮𝐫𝐧𝐞𝐲">𝐁𝐞𝐬𝐭 𝐜𝐨𝐮𝐫𝐬𝐞𝐬, 𝐛𝐨𝐨𝐤𝐬 𝐚𝐧𝐝 𝐫𝐞𝐬𝐨𝐮𝐫𝐜𝐞𝐬 𝐭𝐨 𝐬𝐭𝐚𝐫𝐭/𝐚𝐝𝐯𝐚𝐧𝐜𝐞 𝐲𝐨𝐮𝐫 𝐋𝐋𝐌 𝐚𝐧𝐝 𝐋𝐋𝐌𝐎𝐩𝐬 𝐣𝐨𝐮𝐫𝐧𝐞𝐲:</a></li>
  <li><a href="#-courses" id="markdown-toc--courses">👩‍🏫 Courses</a></li>
  <li><a href="#-curated-list" id="markdown-toc--curated-list">💥 Curated List</a></li>
  <li><a href="#-top-llm-books" id="markdown-toc--top-llm-books">📚 Top LLM Books</a></li>
  <li><a href="#-llm-reading-list" id="markdown-toc--llm-reading-list">📖 LLM Reading List</a>    <ul>
      <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion:</a></li>
    </ul>
  </li>
</ul>

<p><img src="https://cdn-images-1.medium.com/max/2048/1*JeK2nweJRazyOThkJzT_HQ.gif" alt="Image by Author" />
<a href="https://github.com/Abonia1/deeplearningai-generativeAI-short-courses/tree/main"><strong>GitHub - Abonia1/deeplearningai-generativeAI-short-courses: Notebook samples - Generative AI short…</strong>
<em>Notebook samples - Generative AI short courses - deeplearning.ai - Abonia1/deeplearningai-generativeAI-short-courses</em>github.com</a></p>

<h3 id="𝐁𝐞𝐬𝐭-𝐜𝐨𝐮𝐫𝐬𝐞𝐬-𝐛𝐨𝐨𝐤𝐬-𝐚𝐧𝐝-𝐫𝐞𝐬𝐨𝐮𝐫𝐜𝐞𝐬-𝐭𝐨-𝐬𝐭𝐚𝐫𝐭𝐚𝐝𝐯𝐚𝐧𝐜𝐞-𝐲𝐨𝐮𝐫-𝐋𝐋𝐌-𝐚𝐧𝐝-𝐋𝐋𝐌𝐎𝐩𝐬-𝐣𝐨𝐮𝐫𝐧𝐞𝐲">𝐁𝐞𝐬𝐭 𝐜𝐨𝐮𝐫𝐬𝐞𝐬, 𝐛𝐨𝐨𝐤𝐬 𝐚𝐧𝐝 𝐫𝐞𝐬𝐨𝐮𝐫𝐜𝐞𝐬 𝐭𝐨 𝐬𝐭𝐚𝐫𝐭/𝐚𝐝𝐯𝐚𝐧𝐜𝐞 𝐲𝐨𝐮𝐫 𝐋𝐋𝐌 𝐚𝐧𝐝 𝐋𝐋𝐌𝐎𝐩𝐬 𝐣𝐨𝐮𝐫𝐧𝐞𝐲:</h3>

<h2 id="-courses">👩‍🏫 Courses</h2>

<p>👉 Generative AI learning path: <a href="https://www.cloudskillsboost.google/paths/118">https://www.cloudskillsboost.google/paths/118</a></p>

<p>👉 Full Stack LLM Bootcamp : <a href="https://lnkd.in/eH8VXpwB">https://lnkd.in/eH8VXpwB</a></p>

<p>👉 LLM University to learn about LLMs and NLP — Cohere : <a href="https://lnkd.in/etKAjaYg">https://lnkd.in/etKAjaYg</a></p>

<p>👉 Deploying GPT and Large Language Models — Oreilly : <a href="https://lnkd.in/eDDivjB6">https://lnkd.in/eDDivjB6</a></p>

<p>👉 Professional Certificate in Large Language Models- edX : <a href="https://lnkd.in/edg3gPzQ">https://lnkd.in/edg3gPzQ</a></p>

<p>👉 Understanding Large Language Models — Princeton University : <a href="https://lnkd.in/eE44cmza">https://lnkd.in/eE44cmza</a></p>

<p>👉 Natural Language Processing Specialization — coursera : <a href="https://lnkd.in/eNGDYGeA">https://lnkd.in/eNGDYGeA</a></p>

<p>👉 Large Language Models — The Stanford CS324 : <a href="https://lnkd.in/eJKfDTHK">https://lnkd.in/eJKfDTHK</a></p>

<p>👉 Transformers Course — HuggingFace : <a href="https://lnkd.in/eY2-NdGG">https://lnkd.in/eY2-NdGG</a></p>

<p>👉 Large Language Models — Class Central : <a href="https://lnkd.in/exVh6g-K">https://lnkd.in/exVh6g-K</a></p>

<p>👉 Large Language Models — Rycolab : <a href="https://lnkd.in/eRR_EzGW">https://lnkd.in/eRR_EzGW</a></p>

<h2 id="-curated-list">💥 Curated List</h2>

<p>Elevate your Generative AI expertise with a carefully curated selection of Andrew Ng’s GenAI courses. Expand your skill set through focused modules offered by <strong><a href="http://deeplearning.ai/">DeepLearning.AI</a> — <a href="https://www.deeplearning.ai/short-courses/">https://www.deeplearning.ai/short-courses/</a></strong></p>

<p>👉 ChatGPT Prompt Engineering for Developers:
Go beyond the chat box. Use API access to leverage LLMs into your own applications, and learn to build a custom chatbot.
Level : Beginner to Advanced</p>

<p>📌 Link : <a href="https://lnkd.in/gWZUEK2A">https://lnkd.in/gWZUEK2A</a></p>

<p>👉 Building Systems with the ChatGPT API
Level up your use of LLMs. Learn to break down complex tasks, automate workflows, chain LLM calls, and get better outputs.</p>

<p>Level : Beginner to Advanced</p>

<p>📌 Link : <a href="https://lnkd.in/gq9kjmQf">https://lnkd.in/gq9kjmQf</a></p>

<p>👉 LangChain for LLM Application Development
The framework to take LLMs out of the box. Learn to use LangChain to call LLMs into new environments, and use memories, chains, and agents to take on new and complex tasks.</p>

<p>Level : Beginner</p>

<p>📌 Link: <a href="https://lnkd.in/ggpgxHm7">https://lnkd.in/ggpgxHm7</a></p>

<p>👉 LangChain: Chat with Your Data
Create a chatbot to interface with your private data and documents using LangChain.</p>

<p>Level : Beginner</p>

<p>📌 Link : <a href="https://lnkd.in/gUZrfksz">https://lnkd.in/gUZrfksz</a></p>

<p>👉 Building Generative AI Applications with Gradio
Create and demo machine learning applications quickly. Share your app with the world on Hugging Face Spaces.</p>

<p>Level : Beginner</p>

<p>📌 Link : <a href="https://lnkd.in/gCHRS7nv">https://lnkd.in/gCHRS7nv</a></p>

<p>👉 Evaluating and Debugging Generative AI
Learn MLOps tools for managing, versioning, debugging and experimenting in your ML workflow.</p>

<p>Level : Intermediate</p>

<p>📌 Link : <a href="https://lnkd.in/gdZd-prA">https://lnkd.in/gdZd-prA</a></p>

<p>👉 How Diffusion Models Work
Learn and build diffusion models from the ground up. Start with an image of pure noise, and arrive at a final image, learning and building intuition at each step along the way.</p>

<p>Level : Intermediate</p>

<p>📌 Link : <a href="https://lnkd.in/g7ajmY4X">https://lnkd.in/g7ajmY4X</a></p>

<p>👉 Finetuning Large Language Models
Learn to finetune an LLM in minutes and specialize it to use your own data</p>

<p>Level : Intermediate</p>

<p>📌 Link : <a href="https://lnkd.in/ghxMEpCX">https://lnkd.in/ghxMEpCX</a></p>

<p>👉 Functions, Tools and Agents with LangChain
Learn about the most recent advancements in LLM APIs.Use LangChain Expression Language (LCEL), a new syntax to compose and customize chains and agents faster.</p>

<p>Level : Intermediate</p>

<p>📌 Link : <a href="https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/">https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/</a></p>

<p>👉 LLMOps
Learn LLMOps best practices as you design and automate the steps to tune an LLM for a specific task and deploy it as a callable API. In the course, you’ll tune an LLM to act as a question-answering coding expert.</p>

<p>Level : Beginner</p>

<p>📌 Link : <a href="https://learn.deeplearning.ai/courses/llmops/lesson/1/introduction">https://learn.deeplearning.ai/courses/llmops/lesson/1/introduction</a></p>

<p>👉 Advanced Retrieval for AI with Chroma
Learn advanced retrieval techniques to improve the relevancy of retrieved results.</p>

<p>Level : Intermediate</p>

<p>📌 Link : <a href="https://learn.deeplearning.ai/courses/advanced-retrieval-for-ai/lesson/1/introduction">https://learn.deeplearning.ai/courses/llmops/lesson/1/introduction</a></p>

<h2 id="-top-llm-books">📚 Top LLM Books</h2>

<p><img src="https://cdn-images-1.medium.com/max/3368/1*K12j0-zJ-wELcW0gtPWYRQ.png" alt="" /></p>

<p><img src="https://cdn-images-1.medium.com/max/3332/1*AdXyZe1YTMoAQeM1FZEpQg.png" alt="" /></p>

<p>👉 Practical Natural Language Processing — Oreilly : <a href="https://lnkd.in/eKBHvdzM">https://lnkd.in/eKBHvdzM</a></p>

<p>👉 Natural Language Processing with Transformers — Oreilly : <a href="https://lnkd.in/ehazWcMY">https://lnkd.in/ehazWcMY</a></p>

<p>👉 Transformers for Natural Language Processing — Packt : <a href="https://lnkd.in/e_Y5cX6c">https://lnkd.in/e_Y5cX6c</a></p>

<p>👉 GPT-3: Building Innovative NLP Products Using Large Language Models : <a href="https://lnkd.in/eSpvDErp">https://lnkd.in/eSpvDErp</a></p>

<p>👉 Hands-On Generative AI with Transformers and Diffusion Models: <a href="https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/">https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/</a></p>

<p>👉 Quick Start Guide to Large Language Models: Strategies and Best Practices for using ChatGPT and Other LLMs : <a href="https://lnkd.in/erbhdEjU">https://lnkd.in/erbhdEjU</a></p>

<h2 id="-llm-reading-list">📖 LLM Reading List</h2>

<p>👉 Understanding Large Language Models-A Transformative Reading List : <a href="https://lnkd.in/eEv2vi2w">https://lnkd.in/eEv2vi2w</a></p>

<p>👉 Practical Deep Learning for Coders : <a href="https://course.fast.ai/">https://course.fast.ai</a></p>

<p>👉 The Annotated Transformer : <a href="https://lnkd.in/et883Grd">https://lnkd.in/et883Grd</a></p>

<p>👉 The Illustrated Transformer : <a href="https://lnkd.in/ehc9Bpk7">https://lnkd.in/ehc9Bpk7</a></p>

<p>👉 Langchain Demo : <a href="https://lnkd.in/eWyWzZrb">https://lnkd.in/eWyWzZrb</a></p>

<table>
  <tbody>
    <tr>
      <td>👉 State of GPT</td>
      <td>BRK216HFS : <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A&amp;themeRefresh=1">https://www.youtube.com/watch?v=bZQun8Y4L2A&amp;themeRefresh=1</a></td>
    </tr>
  </tbody>
</table>

<p>👉 Awsome LLMOps — GitHub : <a href="https://lnkd.in/e3KNspKi">https://lnkd.in/e3KNspKi</a></p>

<p>👉 Awsome LLM Repo — GitHub : <a href="https://lnkd.in/eR2JwbPV">https://lnkd.in/eR2JwbPV</a></p>

<p>👉 LLM Cheatsheet — Github : <a href="https://github.com/Abonia1/CheatSheet-LLM">https://github.com/Abonia1/CheatSheet-LLM</a></p>

<p>👉 LLMOps — <a href="https://vinija.ai/concepts/LLMOps/">https://vinija.ai/concepts/LLMOps/</a></p>

<p>👉 OpenAI Cookbook — <a href="https://github.com/openai/openai-cookbook">https://github.com/openai/openai-cookbook</a></p>

<h3 id="conclusion">Conclusion:</h3>

<p>I hope you found this compilation of resources on large language models to be beneficial. We have included a variety of courses, books, reading lists, and other valuable resources and frameworks that can assist you in constructing your own impactful applications based on LLMs. Stay tune for another intresting article on LLM.</p>

<p>Keep an eye out for future updates to this list, and stay tuned for another intriguing article on LLM.</p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Curated list of best courses, books, resources on large language model]]></summary></entry><entry><title type="html">YOLOv8 Fire and Smoke Detection</title><link href="http://localhost:4000/blogs/2023-02-10-YOLOv8-Fire-and-Smoke-Detection/" rel="alternate" type="text/html" title="YOLOv8 Fire and Smoke Detection" /><published>2023-02-10T00:00:00+01:00</published><updated>2024-04-15T19:07:11+02:00</updated><id>http://localhost:4000/blogs/YOLOv8-Fire-and-Smoke-Detection</id><content type="html" xml:base="http://localhost:4000/blogs/2023-02-10-YOLOv8-Fire-and-Smoke-Detection/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [Yolov8, Fireandsmokedetection, Machine Learning, Computer Vision, AI]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#yolov8" id="markdown-toc-yolov8">YOLOv8</a></li>
  <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a></li>
  <li><a href="#model-training-and-evaluation" id="markdown-toc-model-training-and-evaluation">Model Training and Evaluation</a></li>
  <li><a href="#inference" id="markdown-toc-inference">Inference</a></li>
  <li><a href="#application-area" id="markdown-toc-application-area"><strong>Application Area</strong></a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>
<p><strong>Table of Contents:</strong></p>
<ul>
  <li><a href="#dataset">Dataset</a></li>
  <li><a href="#model-training-and-evaluation">Model Training and Evaluation</a></li>
  <li><a href="#Inference">Inference</a></li>
  <li><a href="#application-area">Application Area</a></li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://cdn-images-1.medium.com/max/2000/1*HVm63Me_kPRNy0vxekCabw.gif" alt="Media by Author - Live Fire and smoke detection and tracking using YOLOv8" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><i>Image Credits - Media by Author - Live Fire and smoke detection and tracking using YOLOv8</i></td>
    </tr>
  </tbody>
</table>

<p>Fire and smoke detection is a critical task for ensuring public safety and preventing property damage. With recent advances in computer vision and deep learning, it is possible to build accurate fire and smoke detection systems using custom datasets. One such system is YOLOv8, a state-of-the-art object detection model that can be trained on a custom dataset to detect fire and smoke.</p>

<p>This article will redirect you to the <strong><a href="https://github.com/Abonia1/YOLOv8-Fire-and-Smoke-Detection">project</a></strong>.</p>

<p><strong><a href="https://github.com/Abonia1/YOLOv8-Fire-and-Smoke-Detection">This repository contains the code for tracking and detecting fires and smokes in real-time video using YOLOv8.</a></strong></p>

<h2 id="yolov8">YOLOv8</h2>

<p><strong><a href="https://github.com/ultralytics/ultralytics">Ultralytics YOLOv8</a></strong>, developed by <strong><a href="https://ultralytics.com">Ultralytics</a></strong>. YOLOv8 stands for “You Only Look Once version 8” and it is an improvement over the previous YOLO models. It is a single-shot object detection model that can detect multiple objects in an image in real-time. Unlike other object detection models that divide the image into multiple regions and perform object detection on each region, YOLOv8 performs object detection on the entire image in one forward pass through the network. This makes it fast and efficient for real-time applications.</p>

<h2 id="dataset">Dataset</h2>

<p>We have used image dataset from roboflow where it was annoted for to find Fire, smoke and default with boxes.</p>

<p>Link to Fire and Smoke detection <strong><em><a href="https://github.com/Abonia1/YOLOv8-Fire-and-Smoke-Detection/tree/main/datasets/fire-8">dataset</a>.</em></strong></p>

<h2 id="model-training-and-evaluation">Model Training and Evaluation</h2>

<p>The process of training YOLOv8 on a custom dataset involves several steps. First, a dataset of images containing fire and smoke should be collected and annotated. This involves marking the regions of interest in each image that correspond to fire and smoke. The annotated images can then be split into a training set and a validation set for use in the training process.I have useed <strong><em><a href="https://roboflow.com/">roboflow</a></em></strong> for the annotated dataset.</p>

<p><strong>Complete code in Colab Notebook:</strong></p>

<p><strong><a href="https://gist.github.com/Abonia1/eac0e5db887855bbab7875b914ebf429#file-train-yolov8-early-fire-smoke-detection-on-custom-dataset-ipynb"> Notebook Link </a></strong></p>

<script src="https://gist.github.com/Abonia1/eac0e5db887855bbab7875b914ebf429.js"></script>

<p>Once the model is trained, it can be evaluated on the validation set to measure its performance. This can be done by running the evaluation script, which outputs the mean Average Precision (mAP) score for the validation set. The mAP score is a measure of how well the model is able to detect the objects of interest in the validation set.</p>

<p><img src="https://cdn-images-1.medium.com/max/6000/1*8bM1hbqo6yO3-431pTaTAg.png" alt="Source-Image by Author: Confusion Matrix" /></p>

<p><img src="https://cdn-images-1.medium.com/max/4800/1*3-wM1-LMHxSb8PexVczhFg.png" alt="Source-Image by Author: Train,test,validation loss and mAP" /></p>

<p><img src="https://cdn-images-1.medium.com/max/4500/1*xGI8pDGGspE-stuyTfNLvg.png" alt="Source-Image by Author: PR curve" /></p>

<p>Finally, the trained model can be used for inference on new images/videos. This can be done by running the inference script and providing it with the path to an image. The script will display the detected fire and smoke regions on the input image.</p>

<h2 id="inference">Inference</h2>

<p>Sample Results :</p>

<p><img src="https://cdn-images-1.medium.com/max/3840/1*MlsDxcNfuYKBN88T-Y7TzA.jpeg" alt="" /></p>

<p><img src="https://cdn-images-1.medium.com/max/3840/1*0cI6BGRgIkBa7cMOHsn3CA.jpeg" alt="" /></p>

<h2 id="application-area"><strong>Application Area</strong></h2>

<p>Fire and smoke detection have a wide range of applications, including:</p>

<p>1.<strong>Residential and commercial buildings</strong>: Fire and smoke detectors are mandatory in all buildings to ensure the safety of residents and workers. They can alert people in case of a fire and give them enough time to evacuate the building.</p>

<ol>
  <li><strong>Industrial facilities</strong>: Fire and smoke detectors are crucial in industrial facilities where dangerous chemicals or flammable materials are stored. They can prevent fire outbreaks and minimize damage to property and the environment.</li>
</ol>

<p>3.<strong>Public transportation</strong>: Fire and smoke detectors are essential in public transportation such as buses, trains, and airplanes. They can ensure the safety of passengers and prevent disasters.</p>

<ol>
  <li>
    <p><strong>Power plants</strong>: Fire and smoke detectors are used in power plants to detect and prevent fires that can cause damage to critical equipment and disrupt power supply.</p>
  </li>
  <li>
    <p><strong>Military and defense</strong>: Fire and smoke detectors are used in military and defense installations to detect and prevent fires that can damage sensitive equipment and compromise national security.</p>
  </li>
  <li>
    <p><strong>Mining and oil and gas industries:</strong> Fire and smoke detectors are used in mines and oil and gas facilities to detect and prevent fires that can pose a risk to workers and the environment.</p>
  </li>
  <li>
    <p><strong>Forest Fire and Wildfire</strong>: Detect and alert in-case of forest fires.It helps to take the necessary steps to prevent forest fire and its negative effects.</p>
  </li>
</ol>

<p>In addition to these applications, fire and smoke detection systems are also used in museums, archives, and data centers to protect valuable and irreplaceable items. Overall, fire and smoke detection is a crucial component of public safety and disaster preparedness.</p>

<h2 id="conclusion">Conclusion</h2>

<p>YOLOv8 is a powerful tool for building fire and smoke detection systems using custom datasets. With its speed and accuracy, it is a promising solution for real-world applications that require fast and efficient fire and smoke detection.</p>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Fine-Tune YOLOv8 Fire-and-Smoke-Detection on custom data]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn-images-1.medium.com/max/2000/1*HVm63Me_kPRNy0vxekCabw.gif" /><media:content medium="image" url="https://cdn-images-1.medium.com/max/2000/1*HVm63Me_kPRNy0vxekCabw.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>